# Nexus Core ‚Äî Testing Plan

## Overview

This document outlines the comprehensive testing strategy for Nexus Core. The framework has multiple testing levels to ensure reliability, security, and performance.

**Current State:** 59 test files, 828 tests passing, 0 type errors, 0 lint errors

---

## 1. Unit Tests (‚úÖ Complete)

**Status:** 828 tests passing
**Command:** `pnpm test`

### Coverage Areas
- ‚úÖ **Core**: agent-runner, result types, errors
- ‚úÖ **Providers**: anthropic, openai, factory
- ‚úÖ **Tools**: registry, all 7 tool definitions
- ‚úÖ **Memory**: memory-manager, prisma-memory-store
- ‚úÖ **Cost**: cost-guard, usage tracking
- ‚úÖ **Security**: approval-gate, input-sanitizer
- ‚úÖ **Prompts**: prompt-builder, layer-manager
- ‚úÖ **Scheduling**: task-manager, task-runner
- ‚úÖ **API**: all routes (projects, sessions, chat, etc.)
- ‚úÖ **CLI**: chat.ts pure functions

### Run Unit Tests
```bash
pnpm test                    # All tests
pnpm test -- --run <file>   # Single file
pnpm test:unit              # Unit tests only (when integration tests exist)
```

---

## 2. Integration Tests (‚ùå TODO)

**Status:** Not yet implemented
**Command:** `pnpm test:integration` (to be created)

### What to Test

#### 2.1 Database Integration
- [ ] **Project CRUD** ‚Äî Create, read, update, delete projects with real Prisma
- [ ] **Session persistence** ‚Äî Sessions saved and retrieved correctly
- [ ] **Prompt layers** ‚Äî Activate/deactivate layers, version management
- [ ] **Execution traces** ‚Äî Full trace events written and queried
- [ ] **Usage records** ‚Äî Cost tracking persists across runs
- [ ] **Memory entries** ‚Äî Long-term memory with pgvector similarity search

#### 2.2 Redis + BullMQ Integration
- [ ] **Task scheduling** ‚Äî Scheduled tasks enqueue and run via BullMQ
- [ ] **Task execution** ‚Äî Worker processes jobs correctly
- [ ] **Cron parsing** ‚Äî Cron expressions calculate next run times
- [ ] **Task state transitions** ‚Äî proposed ‚Üí approved ‚Üí active ‚Üí paused ‚Üí completed

#### 2.3 Provider Integration (Real APIs)
- [ ] **Anthropic** ‚Äî Real Claude API call with streaming
- [ ] **OpenAI** ‚Äî Real GPT-4o API call with streaming + usage
- [ ] **Cost calculation** ‚Äî Real usage ‚Üí cost with actual model pricing
- [ ] **Failover** ‚Äî Primary provider fails ‚Üí fallback provider used
- [ ] **Rate limiting** ‚Äî Respect provider rate limits

#### 2.4 Tool Execution
- [ ] **calculator** ‚Äî Real math operations
- [ ] **date-time** ‚Äî Real date/time queries
- [ ] **json-transform** ‚Äî Real JSON manipulation
- [ ] **Tool approval** ‚Äî High-risk tools pause for approval
- [ ] **Tool errors** ‚Äî Graceful handling of tool failures

#### 2.5 Memory System
- [ ] **Context window** ‚Äî Pruning when conversation exceeds limit
- [ ] **Compaction** ‚Äî LLM-summarized compression works
- [ ] **Long-term memory** ‚Äî pgvector retrieval returns relevant memories
- [ ] **Memory decay** ‚Äî Decay function reduces old memory scores

### Setup
```bash
# Start Docker services
docker-compose up -d postgres redis

# Run migrations
pnpm db:migrate

# Seed test data
pnpm db:seed

# Run integration tests
pnpm test:integration
```

**TODO:** Create `src/**/*.integration.test.ts` files for above scenarios.

---

## 3. End-to-End Tests (‚ùå TODO)

**Status:** Manual only (no automated E2E yet)
**Scope:** Full agent loop from user input ‚Üí LLM ‚Üí tools ‚Üí response

### 3.1 Chat API E2E
- [ ] POST `/chat` with real LLM provider
  - User message ‚Üí agent response
  - Session continuity across messages
  - Tool use (calculator, date-time, json-transform)
  - Cost tracking updates
  - Trace events written to DB

### 3.2 WebSocket Streaming E2E
- [ ] WS `/chat/stream` with real LLM provider
  - Token-by-token streaming works
  - Tool use events emitted (tool_use_start, tool_result)
  - agent_complete event includes usage + cost
  - Reconnection handling

### 3.3 CLI Chat E2E (‚úÖ Manually Verified)
- [x] `pnpm chat` selects project
- [x] Streaming works in terminal
- [x] Token counts display correctly
- [x] Cost displays with real pricing
- [x] Commands work (/quit, /new, /help)

### 3.4 Scheduled Tasks E2E
- [ ] Agent proposes task via `propose-scheduled-task` tool
- [ ] Task appears as `proposed` status
- [ ] Human approves task via API
- [ ] BullMQ worker picks up task at next cron time
- [ ] Agent runs and completes task
- [ ] Task run saved to DB with trace

### 3.5 Multi-Turn Conversations
- [ ] Agent asks clarifying questions
- [ ] User provides more context
- [ ] Agent uses previous context to answer
- [ ] Memory pruning doesn't break context
- [ ] Max turns limit respected

### 3.6 Error Scenarios
- [ ] Budget exceeded ‚Üí agent stops gracefully
- [ ] Max turns reached ‚Üí agent returns partial answer
- [ ] LLM timeout ‚Üí fallback provider used (if configured)
- [ ] Tool execution fails ‚Üí agent handles error and informs user
- [ ] Invalid tool input ‚Üí agent retries with corrected input

### Run E2E Tests
```bash
# Start services
docker-compose up -d
pnpm dev

# Run E2E suite (TODO: create)
pnpm test:e2e
```

---

## 4. Manual Testing Checklist

### 4.1 CLI Chat (`pnpm chat`)
- [x] Project selection menu
- [x] Streaming token-by-token output
- [x] Token count displays correctly
- [x] Cost displays with real pricing ($0.00XX format)
- [x] Session continuity (follow-up questions use context)
- [x] `/new` command starts new session
- [x] `/help` shows commands
- [x] `/quit` exits cleanly
- [x] ANSI colors render correctly
- [ ] Tool use displays (e.g., calculator)
- [ ] Error messages in red
- [ ] Ctrl+C graceful shutdown

### 4.2 REST API
- [ ] `GET /projects` ‚Äî List all projects
- [ ] `POST /projects` ‚Äî Create project with valid AgentConfig
- [ ] `GET /projects/:id` ‚Äî Get single project
- [ ] `PATCH /projects/:id` ‚Äî Update project config
- [ ] `GET /projects/:id/sessions` ‚Äî List sessions
- [ ] `POST /chat` ‚Äî Send message, get response
- [ ] `GET /projects/:id/prompt-layers/active` ‚Äî Active layers per type
- [ ] `POST /projects/:id/prompt-layers` ‚Äî Create new layer version
- [ ] `POST /prompt-layers/:id/activate` ‚Äî Activate layer, deactivate old
- [ ] `GET /traces/:id` ‚Äî Full execution trace
- [ ] `POST /scheduled-tasks/:id/approve` ‚Äî Approve proposed task

### 4.3 WebSocket Streaming
- [ ] Connect to `ws://localhost:3002/chat/stream`
- [ ] Send `{ projectId, message }`
- [ ] Receive `agent_start` event
- [ ] Receive `content_delta` events (streaming text)
- [ ] Receive `tool_use_start` / `tool_result` (if tool used)
- [ ] Receive `agent_complete` with usage + cost
- [ ] Error events on invalid input

### 4.4 Security & Permissions
- [ ] High-risk tool blocks without approval
- [ ] Approval gate pauses execution
- [ ] POST `/approvals/:id/approve` unblocks
- [ ] POST `/approvals/:id/deny` aborts run
- [ ] Tool not in `allowedTools` is rejected
- [ ] Input sanitizer strips harmful patterns
- [ ] Secrets not exposed in logs or traces

### 4.5 Scheduled Tasks
- [ ] Agent proposes task with cron expression
- [ ] Task appears in `GET /scheduled-tasks?status=proposed`
- [ ] Approve via `POST /scheduled-tasks/:id/approve`
- [ ] Task runs at scheduled time (check logs)
- [ ] Task run saved to DB
- [ ] `POST /scheduled-tasks/:id/pause` prevents execution
- [ ] `POST /scheduled-tasks/:id/resume` re-enables

### 4.6 Cost & Budget
- [ ] Usage records saved after each turn
- [ ] Cost calculated with real model pricing
- [ ] Daily budget exceeded ‚Üí agent stops
- [ ] Monthly budget exceeded ‚Üí agent stops
- [ ] Alert threshold triggers warning log
- [ ] `GET /projects/:id/usage` returns aggregated costs

---

## 5. Performance Testing (‚ùå TODO)

### 5.1 Load Testing
- [ ] 10 concurrent chat requests
- [ ] 50 concurrent chat requests
- [ ] 100 concurrent chat requests
- [ ] Measure: response time, throughput, error rate

### 5.2 Streaming Performance
- [ ] WebSocket streaming latency (time to first token)
- [ ] Token throughput (tokens/sec)
- [ ] Concurrent WebSocket connections

### 5.3 Database Performance
- [ ] Trace write performance (10k+ events)
- [ ] pgvector similarity search (1k+ memories)
- [ ] Session query performance (1k+ sessions)

### 5.4 Memory Limits
- [ ] Context window pruning (200k token conversations)
- [ ] Compaction triggers correctly
- [ ] Long-term memory retrieval speed

### Tools
- **k6** or **Artillery** for HTTP load testing
- **wscat** for WebSocket testing
- **pgbench** for database benchmarking

---

## 6. Security Testing (‚ùå TODO)

### 6.1 Input Validation
- [ ] SQL injection attempts blocked
- [ ] XSS payloads sanitized
- [ ] Command injection rejected
- [ ] Path traversal blocked
- [ ] Excessively long inputs rejected

### 6.2 Tool Security
- [ ] Tools cannot access filesystem
- [ ] Tools cannot execute shell commands
- [ ] Tools cannot access network (except HTTP tool with allowlist)
- [ ] Hallucinated tools rejected by registry

### 6.3 Prompt Injection
- [ ] Agent doesn't leak system prompt
- [ ] Agent doesn't execute user-injected instructions
- [ ] Safety layer enforced even with adversarial prompts

### 6.4 RBAC
- [ ] Tools respect `allowedTools` whitelist
- [ ] Approval gates cannot be bypassed
- [ ] Projects isolated from each other

---

## 7. Regression Testing

### When to Run
- Before every release
- After any core system change (agent-runner, providers, tools)
- After dependency updates

### Checklist
- [ ] All unit tests pass (`pnpm test`)
- [ ] No type errors (`pnpm typecheck`)
- [ ] No lint errors (`pnpm lint`)
- [ ] Manual E2E scenarios pass
- [ ] No performance degradation vs baseline

---

## 8. Continuous Integration (‚ùå TODO)

### GitHub Actions Workflow (TODO)
```yaml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: pgvector/pgvector:pg16
      redis:
        image: redis:7-alpine
    steps:
      - uses: actions/checkout@v3
      - uses: pnpm/action-setup@v2
      - uses: actions/setup-node@v3
        with:
          node-version: 22
      - run: pnpm install
      - run: pnpm typecheck
      - run: pnpm lint
      - run: pnpm test
      - run: pnpm test:integration
```

---

## 9. Test Data Management

### Seeds
- `pnpm db:seed` ‚Äî Demo Project (now with correct config)
- `pnpm db:seed:fomo` ‚Äî Fomo Internal Assistant (OpenAI)

### Fixtures
- `src/testing/fixtures/context.ts` ‚Äî Mock ExecutionContext
- `src/testing/fixtures/routes.ts` ‚Äî Mock Fastify instance

### Test Database
- Use separate test DB: `nexus_core_test`
- Reset between integration test runs
- Seed test data via fixtures

---

## 10. Known Issues & Edge Cases

### Fixed
- ‚úÖ OpenAI usage chunk skipped (fixed by capturing chunk.usage separately)
- ‚úÖ Demo Project config wrong shape (fixed by migration script)
- ‚úÖ Cost always $0.0000 (fixed by importing real pricing from models.ts)

### To Investigate
- [ ] MaxListenersExceededWarning on server start (11 exit listeners)
- [ ] tsx watch DLL lock issue (EPERM on Prisma generate while server running)
- [ ] Windows path separators in file storage (use `/` not `\`)

---

## Summary

| Test Level        | Status      | Coverage |
|-------------------|-------------|----------|
| Unit Tests        | ‚úÖ Complete | 828 tests |
| Integration Tests | ‚ùå TODO     | 0%       |
| E2E Tests         | üü° Manual   | CLI only |
| Performance Tests | ‚ùå TODO     | 0%       |
| Security Tests    | ‚ùå TODO     | 0%       |

**Next Steps:**
1. ‚úÖ Fix CLI chat + token usage (DONE)
2. ‚úÖ Fix cost calculation (DONE)
3. ‚úÖ Fix Demo Project config (DONE)
4. ‚ùå Create integration tests for DB + Redis + providers
5. ‚ùå Automate E2E tests for full agent loop
6. ‚ùå Set up CI/CD pipeline with automated tests
